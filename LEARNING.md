

# ✅**1. 软件测试导论**

## **Q1. 什么是软件测试？其重要性何在？**

**答案：**

软件测试是评估软件以发现缺陷并确保其满足需求的过程。

**重要性：**提升质量、降低风险、验证功能、增强可靠性。

**核心概念：**

- 测试=质量保证，而非仅是发现缺陷。
- 测试提供**信心**，而非证明。

**常见误区：**

误以为测试能确保**零缺陷**——这不可能。

---

## **Q2. 缺陷、错误、故障、漏洞的区别？**

- **错误**→ 人为失误（开发者理解偏差）。
- **缺陷/漏洞**→ 代码问题。
- **故障**→ 系统行为偏离预期结果。

**常见误区：**

混淆“错误”与“缺陷”。

---

## **Q3. 软件测试的目标？**

- 发现缺陷
- 增强对产品质量的信心
- 验证需求
- 防止生产环境故障
- 提升产品可靠性

---

## **Q4. 为何测试无法证明无缺陷？**

因为无法测试**所有组合**或场景。

**重要提示：**

测试**能降低风险**，但永远无法消除风险。

---

## **Q5. 何谓"杀虫剂悖论"？**

重复相同测试最终将无法发现新缺陷。

**解决方案：**

定期更新测试用例。

---

---

# ✅**2. 单元测试**

## **Q6. 什么是单元测试？**

**对最小可测试单元**（函数、组件）进行隔离测试。

---

## **Q7. 存根与模拟对象有何区别？**

- **存根：**返回固定值 → 模拟依赖模块。
- **模拟对象：**验证交互行为 → 追踪调用与行为。

**易混淆点：**

学生常将二者混淆。

**请记住：**mock = 验证行为。

---

## **Q8. 常用单元测试框架？**

- JUnit
- pytest
- NUnit
- Jasmine / Jest（用于 JavaScript）

---

## **Q9. 什么是TDD？**

测试驱动开发 =**先写测试**，后写代码。

步骤：

1. 红色（失败）
2. 绿（通过）
3. 重构

---

## **Q10. 为何单元测试通常采用自动化？**

- 快速
- 可重复
- 一致
- 易于集成到CI/CD流程

---

---

# ✅**3. 集成测试**

## **Q11. 什么是集成测试？**

**在单元测试之后，**测试模块之间的交互。

---

## **Q12. 自顶向下与自底向上？**

- **自上而下：**从顶层模块开始测试 → 使用存根。
- **自底向上：**从低层模块开始测试 → 使用驱动程序。

---

## **Q13. 什么是驱动程序和存根？**

- **驱动程序**→ 调用低层级模块（用于自下而上）
- **存根**→ 虚拟模块（用于自顶向下）

---

## **Q14. 增量集成更适合的场景？**

大型银行系统 → 一次性全面集成风险过高。

增量集成可降低调试复杂度。

---

## **Q15. 何谓混合集成？**

结合自顶向下与自底向上方法。

---

---

# ✅**4. 系统测试**

## **Q16. 什么是系统测试？**

对整个系统进行端到端测试。

---

## **Q17. 非功能性测试的类型？**

- 性能
- 安全性
- 可用性
- 兼容性
- 可靠性

---

## **Q18. 端到端测试的目的？**

模拟**真实用户**在整个应用程序中的**工作流程**。

---

## **Q19. 为何系统测试必须模拟生产环境？**

以反映实际运行时的行为。

---

## **Q20. 什么是配置测试？**

测试不同配置：浏览器版本、操作系统、硬件设备。

---

---

# ✅**5. 验收测试**

## **Q21. 什么是验收测试？**

由**客户**执行的最终测试，用于验证是否满意。

---

## **Q22. UAT与Alpha测试的区别？**

- **UAT（用户验收测试）：**由真实用户执行。
- **Alpha测试：**在正式发布前由内部人员执行。

---

## **Q23. 什么是Beta测试？**

向外部用户发布以获取实际使用反馈。

---

## **Q24. 为何验收测试需由客户执行？**

因为只有客户才能验证业务价值和真实需求。

---

## **Q25. 验收标准示例？**

“用户必须能在1分钟内重置密码。”

---

---

# ✅**6. 软件开发过程**

## **Q26. 什么是V模型？**

每个开发阶段都对应一个测试阶段。

---

## **Q27. 敏捷开发中的质量保证？**

- 持续测试
- 测试人员参与需求清单管理与用户故事细化
- 测试自动化
- 与开发人员配对测试

---

## **Q28. 瀑布式与敏捷测试对比？**

- **瀑布式：**后期测试，文档繁琐
- **敏捷：**早期测试，迭代灵活

---

## **Q29. 什么是左移测试？**

将测试环节前移至开发周期早期。

---

## **Q30. 什么是持续测试？**

将测试集成到持续集成/持续交付（CI/CD）管道中。

---

---

# ✅**7. 测试用例**

## **Q31. 什么是测试用例？**

**答案：**

用于验证特定行为的一组条件、输入、执行步骤和预期结果。

**核心组成部分：**

- 测试用例ID
- 标题
- 先决条件
- 测试步骤
- 测试数据
- 预期结果
- 实际结果（后续执行）
- 状态

**易出错点：**

初学者常忽略*先决条件*和*测试数据。*

---

## **Q32. 什么是测试套件？**

按功能分组的测试用例集合（例如登录测试套件）。

---

## **Q33. 什么是测试场景？**

对测试内容的高层级描述，不包含具体步骤细节。

示例：“验证用户能否成功登录”。

---

## **Q34. 登录页面的测试用例示例。**

- **标题：**使用有效凭证登录
- **步骤：**
    1. 输入有效邮箱
    2. 输入有效密码
    3. 点击登录
- **预期结果：**跳转至主页

---

## **Q35. 预期结果与实际结果对比？**

- **预期：**理想情况下应发生的情况
- **实际：**执行过程中发生的情况

---

**重要概念：**

- 测试用例必须**清晰**、**可重复且** **无歧义**。
- 优质测试用例不应相互依赖。

---

---

# ✅**8. 测试用例设计技术**

## **Q36. 什么是等价类划分？**

将输入划分为各组，使所有值表现一致。

示例：年龄字段 1–120 → 分区：

- 无效：<1
- 有效：1–120
- 无效：>120

---

## **Q37. 何谓边界值分析？**

在边界处测试值：

若1–120有效 → 测试：

- 0
- 1
- 120
- 121

**易出错点：**

学生容易忘记**临界值**。

---

## **Q38. 何谓决策表测试？**

一种基于规则与结果表格的测试方法，尤其适用于复杂业务逻辑。

---

## **Q39. 状态转换测试？**

测试系统在状态转换过程中的行为表现。

示例：

ATM

- 状态：“插入卡片”→“输入密码”。

---

## **Q40. 什么是成对测试？**

测试所有输入对组合而非全部组合。

当输入组合数量激增时（如7个参数）尤为有效。

---

### 重要概念

- **黑盒技术：**输入点测试、边界值分析、决策表、状态转换、成对测试。
- **白盒技术：**代码覆盖率、路径测试。

---

### 易错点

- 忽略无效状态。
- 仅测试成功路径，未涵盖负面案例。

---

---

# ✅**9. 探索性测试**

## **Q41. 何谓探索性测试？**

在无预定义脚本的情况下，同步进行学习、测试设计与执行。

---

## **Q42. 与脚本化测试的区别？**

- 脚本化 = 预先编写的测试用例
- 探索式 = 测试人员在测试过程中决定

---

## **Q43. 什么是测试章程？**

探索性测试环节的任务或目标。

示例：

"探索登录页面的可用性问题"

---

## **Q44. 探索性测试人员需要哪些关键技能？**

- 批判性思维
- 领域知识
- 创造力
- 观察力
- 好奇心

---

## **Q45. 什么是基于会话的测试管理？**

一种结构化的探索性测试方法，具有以下特点：

- 时间盒式会话
- 目标
- 记录
- 总结会议

---

**易误解的常见误区：**

探索性测试 ≠ 随机测试。它具有结构性与目的性。

---

---

# ✅**10. 动态测试**

## **Q46. 何谓动态测试？**

通过执行代码进行测试。

---

## **Q47. 黑盒测试与白盒测试有何区别？**

- **黑盒测试：**无需了解内部代码 → 侧重输入/输出
- **白盒测试：**需掌握代码知识 → 侧重路径、条件分析

---

## **Q48. 什么是代码覆盖率？**

衡量测试过程中哪些代码行/分支被执行的指标。

类型：

- 行覆盖率
- 分支覆盖率
- 路径覆盖率

---

## **Q49. 什么是性能测试？**

测试速度、响应能力、负载下的稳定性。

类型：

- 负载测试 - 测试系统在预期负载下的表现
- 压力测试 - 测试系统在极限负载下的表现
- 浸泡测试 - 测试系统在长时间运行下的稳定性
- 尖峰测试 - 测试系统在突然负载激增时的表现

---

## **Q50. 移动应用中动态测试的示例？**

测试导航、手势、网络请求、电池使用情况。

---

**重要概念：**

动态测试始终运行软件；静态测试则不运行。

---

### 易出错点

- 将压力测试与负载测试混淆
- 认为高覆盖率等于高质量（并非总是如此）

---

---

# ✅**11. 风险分析**

## **Q51. 何为基于风险的测试？**

根据风险（发生概率×影响程度）优先级安排测试。

---

## **Q52. 项目风险与产品风险有何区别？**

- **项目风险：**进度、资源、预算
- **产品风险：**产品中的功能性、性能、安全性风险

---

## **Q53. 发生概率与影响程度有何区别？**

- **可能性：**某项失败的概率
- **影响：**失败时造成的损害程度

---

## **Q54. 高风险功能示例？**

支付处理 → 影响极大且发生概率中等。

---

## **Q55. 何为风险缓解？**

为降低风险采取的措施。

示例：为核心功能添加自动化测试。

---

**易出错点：**

风险 = 不仅是“概率”，还包括**严重程度**。

---

---

# ✅**12. 测量（指标）**

## **Q56. 什么是软件测试指标？**

**答案：**

用于评估测试进度、质量和有效性的定量衡量标准。

示例：

- 已执行测试用例数量
- 测试覆盖率
- 缺陷密度
- 缺陷严重性分布

---

## **Q57. 什么是缺陷密度？**

每单位软件规模的缺陷数量，例如：

$$
\text{缺陷密度} = \frac{\text{缺陷数量}}{\text{代码行数或功能点数}}
$$

---

## **Q58. 什么是测试覆盖率？**

测试覆盖了软件的多少部分（代码或需求）。

---

## **Q59. 什么是缺陷漏检率？**

衡量测试中遗漏、最终由客户发现的缺陷数量。

$$
\text{泄漏率} = \frac{\text{发布后发现的缺陷数}}{\text{总缺陷数}}
$$

---

## **Q60. 什么是测试有效性？**

测试发现缺陷的效率。

---

### **重要概念**

- 指标必须具有**实质意义**，而非单纯数字。
- 高覆盖率 ≠ 高质量。

### **易出错点**

- 误用指标来"评判测试人员表现"。
- 混淆缺陷密度与缺陷严重性。

**缺陷密度**与**缺陷严重性**的区别：

**缺陷密度**

- 衡量每单位软件规模中的缺陷数量
- 计算公式：缺陷数量 ÷ 代码行数或功能点数
- 用于评估代码质量和测试覆盖的**数量指标**

**缺陷严重性**

- 描述单个缺陷对系统的**影响程度**
- 通常分为不同等级(如严重、高、中、低)
- 用于确定缺陷修复的**优先级**

**核心区别：**

- 缺陷密度关注"**有多少**"缺陷
- 缺陷严重性关注"**多严重**"缺陷

---

---

# ✅**13. 测试自动化**

## **Q61. 何谓测试自动化？**

使用工具/脚本自动运行测试，取代人工执行。

---

## **Q62. 为何要自动化测试？**

- 更快反馈
- 可重复性
- 长期成本效益
- 支持持续集成/持续交付
- 减少人为错误

---

## **Q63. 哪些类型的测试应实现自动化？**

- 回归测试
- 烟雾测试
    
    **Smoke Testing**。
    
    - **定义**：在拿到一个新版本（新构建）后，先做一轮**范围很小、速度很快**的关键功能检查，确认这个版本“基本能跑”，值得继续做更深入的测试。
    - **目的**：快速发现“致命问题”（比如程序起不来、登录不了、核心页面打不开），避免把时间花在一个根本不可用的版本上。
    - **特点**：
        - 覆盖面浅，但覆盖**最核心路径**
        - 通常是**冒烟级别的通过/不通过**判断
        - 可以手动做，也常做成自动化（尤其在 CI/CD 里）
    
    **例子（电商网站）**：能打开首页 → 能注册/登录 → 能搜索商品 → 能加入购物车 → 能进入结算页面。只要这里有一项挂了，通常就先退回修复，而不是继续做详细功能测试。
    
- API测试
- 重复性任务

**避免自动化：**一次性或界面操作繁琐的不稳定测试。

---

## **Q64. 什么是测试自动化框架？**

支持自动化的结构化环境，例如：

- Selenium
- Cypress
- Playwright
- 机器人框架
- JUnit

---

## **Q65. 自动化投资回报率是多少？**

投资回报率 = 收益 / 成本（工具 + 维护）。

---

### **易出错点**

- 认为自动化可完全替代人工测试。
- 试图全面自动化（不切实际）。
- 低估维护成本。

---

---

# ✅**14. 基于模型的测试（MBT）**

## **Q66. 何谓基于模型的测试？**

测试用例由**模型**（状态机、流程图、UML）自动生成。

---

## **Q67. MBT中的模型是什么？**

系统行为的正式表示形式，例如：

- 状态转换模型
- 活动图
- 流程图
- 决策表

---

## **Q68. MBT的优势？**

- 更快的测试生成
- 更佳覆盖率
- 减少人为错误
- 需求变更时更易维护

---

## **Q69. 什么是状态机模型？**

一种表示系统状态及事件触发状态转换的模型。

---

## **Q70. MBT的适用场景示例。**

ATM或电梯系统——具有大量状态与转换。

---

### **易出错点**

- 将状态机测试与探索性测试混淆。
    
    **状态机测试**是基于状态机模型的测试方法，属于基于模型的测试（MBT）的一种应用。
    
    **核心概念：**
    
    - 使用状态机模型来表示系统的状态以及事件触发的状态转换
    - 根据状态转换关系设计测试用例，验证系统在不同状态间的切换是否正确
    
    **适用场景：**
    
    特别适合具有大量状态与转换的系统，例如ATM或电梯系统。
    
    **注意事项：**
    
    容易与探索性测试混淆，但两者是完全不同的测试方法——状态机测试基于预定义的状态模型，而探索性测试则是同时设计和执行测试的动态方法。
    
- 认为MBT仅适用于自动化测试（手动测试同样可行）。

---

---

# ✅**15. 人工智能测试**

## **Q71. 何谓人工智能测试？**

运用人工智能技术实现：

- 提升测试自动化效率
- 生成测试用例
- 检测异常
- 预测缺陷

---

## **Q72. "测试用AI"与"测试AI"有何区别？**

- **测试用AI：**利用AI增强测试能力（例如UI自动化中的对象识别）。
- **测试AI：**确保AI模型行为正确（准确性、偏见、鲁棒性）。
    
    这是测试AI系统时需要关注的三个关键方面：
    
    - **准确性**：AI模型的预测或输出结果是否正确
    - **偏见**：AI系统是否存在数据偏差或不公平的判断
    - **鲁棒性**：AI系统在面对异常输入或变化环境时的稳定性和可靠性
    
    这些是确保AI模型行为正确性的重要测试维度。
    

---

## **Q73. 测试人工智能系统存在哪些特殊挑战？**

- **非确定性输出**：AI系统的输出结果可能不是固定的，同样的输入在不同时间可能产生不同的输出
- **持续学习**：AI模型会不断学习和更新，导致其行为随时间变化
- **数据偏差**：训练数据中可能存在偏见，影响AI系统的公平性和准确性
- **可解释性问题**：很多AI模型（特别是深度学习模型）的决策过程难以解释和理解

---

## **Q74. 何谓模型漂移？**

当真实世界数据发生变化时，AI的准确性会随时间推移而下降。

---

## **Q75. 什么是对抗性测试？**

通过输入误导性数据（如含噪声的图像）来欺骗AI系统。

---

### **易出错点**

- 误以为传统通过/失败测试适用于AI。
    - **传统通过/失败测试不适用于AI**：传统软件测试通常有明确的预期结果——要么通过，要么失败。但AI系统的输出往往是**概率性的、非确定性的**。例如，同一个输入可能产生不同但都合理的输出。因此需要用准确率、召回率等指标来评估，而不是简单的通过/失败判断。
    
    **召回率（Recall）**是评估AI模型性能的重要指标之一。
    
    它衡量的是：在所有实际为正例的样本中，模型正确识别出来的比例。
    
    **公式：**
    
    召回率 = 正确识别的正例数 / 实际正例总数
    
    **为什么重要：**
    
    在测试AI系统时，不能简单用传统的"通过/失败"来判断，而需要用准确率、召回率等指标来评估模型表现。
    
    **实际应用场景：**
    
    比如疾病检测系统，召回率高意味着能找出更多真正患病的人，避免漏诊。
    
- 在AI测试中忽略数据质量问题。
    - **数据质量问题**：AI系统的表现高度依赖训练数据。如果数据存在偏差、不完整或质量差，会直接影响AI的准确性和公平性。测试AI时必须关注数据质量、数据偏见等问题，这是传统软件测试中不太强调的方面。

简单说：测试AI需要新的思维方式，不能套用传统方法，且必须高度重视数据本身的质量。

---

---

# ✅**16. 文档记录**

## **Q76. 常见测试文档有哪些？**

- 测试计划
- 测试策略
- 测试用例
- 测试报告
- 可追溯矩阵

---

## **Q77. 什么是测试计划？**

描述**测试方法、范围、进度、资源和风险的**文档。

---

## **Q78. 什么是测试策略？**

高层次方法论：测试内容、测试方法及工具选型。

---

## **Q79. 什么是可追溯性矩阵？**

连接需求 → 测试用例 → 缺陷。

- 你可以检查 **每条需求有没有对应测试**（避免漏测）。
- 出现缺陷时能快速知道 **影响了哪些需求**（评估影响范围）。
- 需求变更时能快速找到 **要更新哪些测试用例**（维护成本更低）。

---

## **Q80. 什么是测试报告？**

测试活动、覆盖率、发现问题及建议的汇总。

---

### **易出错点**

- 混淆*测试计划*（项目级）与测试*策略*（组织级）。
- 需求变更时忘记更新文档。

---

---

# ✅**17. 错误报告/缺陷报告**

## **Q81. 什么是缺陷报告？**

描述已发现缺陷的文档，包含复现步骤、预期结果与实际结果、运行环境及严重程度。

---

## **Q82. 缺陷报告的关键字段？**

- 标题
- 描述
- 复现步骤
- 预期结果与实际结果对比
- 严重性
- 优先级
- 环境
- 附件/日志

---

## **Q83. 严重性与优先级有何区别？**

- **严重性：**缺陷的严重程度
- **优先级：**修复的紧急程度

示例：

- 严重性高，优先级低 → 罕用功能中发生崩溃

**举个例子：**

假设一个管理后台有个"导出历史日志"功能，这个功能点击后会导致整个系统崩溃（严重性高）。但实际上这个功能一年只有几个管理员用一两次（罕用），所以可以先修复其他影响更多用户的问题（优先级低）。

**核心区别：**

- **严重性**关注的是"影响有多大"
- **优先级**关注的是"多紧急要修"

---

## **Q84. 何为"无法复现"状态？**

测试人员报告了缺陷，但开发人员因缺少步骤/环境不匹配而无法复现。

---

## **Q85. 何谓根本原因分析（RCA）？**

找出缺陷发生的根本原因。

---

### **易出错点**

- 混淆严重性与优先级
- 重现步骤不完善 → 导致修复延迟

---

---

# ✅**18. 静态测试**

## **Q86. 什么是静态测试？**

无需执行代码的测试。

包括：

- 代码审查
- 演练
- 检查
- 静态分析工具

---

## **Q87. 静态测试的优势？**

- 早期发现缺陷
- 修复成本低
- 提升代码质量
- 防止逻辑错误

---

## **Q88. 什么是代码审查？**

开发者检查他人代码的缺陷、可读性及标准遵循情况。

---

## **Q89. 什么是静态分析？**

自动化工具检测代码问题（如SonarQube、ESLint、Pylint）。

---

## **Q90. 静态测试早期发现缺陷的示例。**

未初始化变量、未使用的导入项、不可达代码。

---

### **易出错点**

- 误认为静态测试可替代动态测试
- 误以为静态工具能捕获所有逻辑错误

---

---

# ✅**19. 安全测试**

## **Q91. 何为安全测试？**

测试旨在确保软件能保护数据并抵御攻击。

---

## **Q92. 常见的安全测试类型？**

- 漏洞扫描
- 渗透测试
- 身份验证测试
- 授权测试
- SQL注入测试
- 跨站脚本攻击测试
- CSRF 测试

---

## **Q93. 什么是SQL注入？**

攻击者通过输入字段插入恶意SQL来操纵数据库。

---

## **Q94. 什么是XSS（跨站脚本攻击）？**

将恶意JavaScript注入网页。

---

## **Q95. 什么是渗透测试？**

通过模拟网络攻击来识别系统漏洞。

---

## **Q96. 身份验证与授权有何区别？**

- **身份验证：**你是谁？（登录）
- **授权：**你可访问什么？（权限）

---

## **Q97. 什么是CSRF？**

跨站请求伪造 → 攻击者诱骗用户执行非预期操作。

---

## **Q98. 什么是暴力破解攻击测试？**

检测系统在反复密码尝试下的抗攻击能力。

---

## **Q99. 什么是加密测试？**

验证敏感数据是否以安全方式存储和传输。

---

## **Q100. 安全配置错误示例。**

未修改默认管理员密码。

---

### **易出错点**

- 混淆身份验证与授权
- 误以为仅HTTPS即等于安全
- 忘记测试注销/会话超时
- 未测试密码重置流程

---

---

# **总结表**

## ✅**1. 导论**

- 测试 = 评估软件以发现缺陷 + 确保其满足需求。
- **错误：**人为失误 →**缺陷/漏洞：**代码问题 →**故障：**系统行为异常。
- 测试可降低风险，但**无法保证零缺陷**。
- **农药悖论：**需定期更新测试用例。

---

## ✅**2. 单元测试**

- 测试最小单元（函数/类）。
- 自动化、快速，用于持续集成/持续交付。
- **存根：**模拟被调用模块。
- **模拟对象：**验证交互行为。
- TDD循环：红→绿→重构。
    - **红**：先写一个会失败的测试
    - **绿**：写最少的代码让测试通过
    - **重构**：在保持测试通过的前提下，优化和改进代码质量
    
    重构的目的是在不改变代码功能的情况下，提升代码的可读性、可维护性和结构，消除重复代码，改善设计。
    

---

## ✅**3. 集成测试**

- 测试模块间的交互。
- **自顶向下：**使用存根；**自底向上：**使用驱动程序。
- 大型系统中增量测试优于大爆炸式测试。
    
    **增量测试（Incremental testing / Incremental integration testing）** 和 **大爆炸式测试（Big-bang integration testing）**。
    
    ### 1) 增量测试（增量集成/增量集成测试）
    
    把系统**分成多个可控的小步**来集成与测试：一次只把一部分模块接起来，测通过后再接下一部分。
    
    - 核心特点：**分阶段、逐步扩大范围**
    - 结果：问题更容易定位，风险更可控，能更早得到可运行的“部分系统”。
    
    ### 2) 大爆炸式测试（大爆炸集成测试）
    
    等各模块都开发完，再把所有模块**一次性全部集成**，然后开始测试。
    
    - 核心特点：**一次性全量集成**
    - 结果：一旦出问题，可能是任意模块/接口/配置导致，定位困难，失败成本高。

---

## ✅**4. 系统测试**

- 对整个系统进行端到端测试。
- 包含功能性+非功能性（性能、可用性、安全性）测试。
- 在接近生产环境的条件下执行。

---

## ✅**5. 验收测试**

- 验证系统是否符合业务需求。
- **UAT：**真实用户；**Alpha：**内部用户；**Beta：**外部用户。
- 基于验收标准进行。

---

## ✅**6. 软件开发过程**

- **V模型：**验证 ↔ 确认。
    
    在 **V 模型**里，开发流程左边每个阶段，右边都有一个与之“配对”的测试/评估活动。
    
    ### 1) 验证（Verification）
    
    **主要在 V 的左边（开发阶段），贯穿从需求到设计到编码的各层“对照规格做检查”。**
    
    典型是这些活动：
    
    - 需求评审（需求是否清晰、无矛盾、可测试）
    - 架构/设计评审（设计是否满足需求与约束）
    - 代码审查、静态分析
    - 以及很多课程里也会把**单元测试**看作偏“验证：实现是否符合设计/接口约定”
    
    ### 2) 确认（Validation）
    
    **主要在 V 的右边（测试阶段），用运行系统的方式确认“做出来的东西对用户有用”。**
    
    典型是：
    
    - 系统测试（从用户视角看功能和非功能是否满足）
    - 验收测试 UAT（客户/用户确认是否可接受、是否满足业务目标）
    
    ### “↔” 在 V 模型里怎么对应？
    
    典型对应关系（左边开发阶段 ↔ 右边测试阶段）：
    
    - **需求分析** ↔ **验收测试（确认用户需求被满足）**
    - **系统设计** ↔ **系统测试**
    - **架构/模块设计** ↔ **集成测试**
    - **编码实现** ↔ **单元测试**
    
    **V模型用“验证”和“确认”把开发阶段与测试阶段一一配对**，强调“越早检查越省成本”，并且既要符合规格（验证），也要满足真实需求（确认）。
    
- **敏捷：**持续测试，左移策略。
    
    **左移策略**（Shift-Left Testing）是指在软件开发生命周期中尽早开始测试活动的策略。
    
    **核心思想：**
    
    - 将测试活动从传统的开发后期"左移"到开发早期阶段
    - 越早发现缺陷，修复成本越低
    - 在需求分析、设计阶段就开始进行测试和验证
    
    **在敏捷开发中的体现：**
    
    - 持续测试贯穿整个开发过程
    - 开发人员和测试人员同步协作
    - 单元测试、集成测试在开发过程中就不断执行
    - 通过CI/CD实现自动化的持续测试
    
    这与传统瀑布模型中"先开发完再测试"的方式形成对比，能够更早发现问题、降低修复成本、提升软件质量。
    
- CI/CD → 持续测试。

---

## ✅**7. 测试用例**

- 组件：ID、标题、步骤、输入、预期结果、先决条件。
- 测试套件 = 测试用例集合。
- 测试场景 = 高级用户流程。

---

## ✅**8. 测试用例设计**

- **等价类划分（EP）**：有效/无效组。
- **边界值分析（BVA）**：边界点（min−1, min, max, max+1）。
- **决策表：**复杂逻辑。
- **状态转换：**基于状态的行为。
- **成对测试：**输入组合简化。

---

## ✅**9. 探索性测试**

- 同步学习+设计+执行。
- 采用**测试章程**和**基于会话的测试**。
- 有目的性而非随机。

---

## ✅**10. 动态测试**

- 运行代码。
- **黑盒测试：**输入/输出；**白盒测试：**内部结构。
- 覆盖率：行覆盖、分支覆盖、路径覆盖。
- 包含性能测试。

---

## ✅**11. 风险分析**

- 风险 =**发生概率 × 影响程度**。
- **产品风险：**软件缺陷。
- **项目风险：**进度、资源。
- 基于风险的测试 → 优先处理关键领域。

---

## ✅**12. 度量**

- **缺陷密度**= 缺陷数/千行代码。
- **覆盖率：**需求或代码。
- **缺陷漏检：**发布后发现的缺陷。
- 指标反映进度，而非测试人员表现。

---

## ✅**13. 测试自动化**

- 最适合回归测试、烟雾测试和API测试。
- 并非所有测试都应自动化。
- 常用框架：Selenium、Cypress、Playwright。
- 维护成本高 → 需审慎管理。

---

## ✅**14. 模型驱动测试**

- 从模型自动生成测试用例。
- 模型类型：状态机、流程图、UML。
- 适用于复杂系统（ATM、电梯）。

---

## ✅**15. 人工智能测试**

- **人工智能在测试中的应用：**AI提升自动化水平。
- **测试AI：**验证机器学习模型的准确性、偏见性与鲁棒性。
- 挑战：非确定性、模型漂移、偏见。

---

## ✅**16. 文档管理**

- 文档：测试计划、测试策略、测试用例、可追溯性矩阵、测试报告。
- 测试计划=项目级；测试策略=组织级。
- 可追溯性链接需求 → 测试用例 → 缺陷。

---

## ✅**17. 错误/缺陷报告**

- 缺陷报告字段：重现步骤、预期/实际行为、严重性、优先级、环境。
- **严重性：**影响程度；**优先级：**紧急程度。
- RCA = 查找缺陷根源。

---

## ✅**18. 静态测试**

- 无需执行：代码审查、走查、检查、静态分析工具。
- 早期缺陷检测 → 更低修复成本。
- 工具：SonarQube、ESLint、Pylint。

---

## ✅**19. 安全测试**

- 检查防御攻击的能力。
- 测试SQL注入、跨站脚本攻击、跨站请求伪造、身份验证/授权。
- 渗透测试模拟攻击场景。
- 验证加密机制、会话管理及暴力破解防护措施。

---

---

# **关键概念**

- BVA与EP
- 严重性与优先级
- 测试类型（单元→集成→系统→验收）
- 静态测试与动态测试
- 测试用例结构
- 风险 = 发生概率 × 影响程度
- 自动化投资回报率
- 代码覆盖率类型
- 身份验证与授权
- Alpha测试 vs Beta测试 vs UAT

---

### 1) BVA 与 EP

- **EP（等价类划分）**：把输入按“行为相同”分组，每组测一个代表值。
    - 分 **有效类** 和 **无效类**，目的是 **减少用例数量**。
- **BVA（边界值分析）**：专测边界附近（min、max 及其前后 1）的值。
    - 抓 **off-by-one**、< 和 ≤ 写错这类 bug。
- **关系**：EP 决定“测哪些类”，BVA 决定“边界点测哪些值”。

---

### 2) 严重性（Severity）与优先级（Priority）

- **严重性**：这个缺陷对系统/用户的 **影响有多大**（崩溃、数据丢失通常高）。
- **优先级**：这个缺陷 **多快必须修**（受发布时间、用户量、业务影响决定）。
- **严重性高不一定优先级高**（比如很少人用的功能崩溃）。

---

### 3) 测试类型：单元 → 集成 → 系统 → 验收

- **单元测试**：测最小单元（函数/类），通常隔离依赖。
- **集成测试**：测模块之间接口与交互。
- **系统测试**：把整个系统当成黑盒做端到端（功能 + 非功能）。
- **验收测试**：从客户/用户角度确认“是否满足业务需求/验收标准”。
- 层级越往后，**更接近真实环境**，但定位问题通常更难、成本更高。

---

### 4) 静态测试与动态测试

- **静态测试**：不运行代码（评审、走查、静态分析）。
- **动态测试**：运行软件来测（功能测试、性能测试等）。
- 静态测试能 **更早发现缺陷**、修复成本低，但不能替代动态测试。

---

### 5) 测试用例结构（Test Case 组成）

常见字段：

- **ID、标题/目的**
- **前置条件**
- **步骤**
- **测试数据/输入**
- **预期结果**
- （执行后补）**实际结果、状态、备注/附件**

---

### 6) 风险 = 发生概率 × 影响程度

- **风险**：某件坏事发生的可能性 * 发生后的损失。
    - 常用公式：风险值 = **概率** × **影响**
- 用它做 **基于风险的测试排序**：先测高风险功能（例如支付）。

---

### 7) 自动化投资回报率（ROI）

- **ROI**：自动化带来的收益是否大于成本。
    - 成本：脚本开发 + 环境 + 维护
    - 收益：节省人力时间 + 更快反馈 + 回归更稳定
- 最适合自动化的是 **回归、烟雾、重复性高、结果稳定** 的测试。

---

### 8) 代码覆盖率类型

- **行覆盖率（Statement/Line）**：代码行是否被执行过。
- **分支覆盖率（Branch）**：if/else 每个分支是否走到。
- **路径覆盖率（Path）**：不同执行路径组合是否覆盖（最难做到高）。
- **覆盖率高 ≠ 测试质量高**（可能只是在走“无意义路径”）。
    - 覆盖率指标回答的是：**跑到哪里**
    - 测试质量更关心：**有没有验证对的东西、有没有覆盖高风险与边界、断言有没有力量**

---

### 9) 身份验证（Authentication）与授权（Authorization）

- **身份验证**：你是谁（登录、验证身份）。
- **授权**：你能做什么（权限控制）。
- 先验证身份，才能谈授权；常见安全问题是 **越权访问**。

---

### 10) Alpha vs Beta vs UAT

- **Alpha 测试**：内部做，发布前，环境/参与者通常在公司内。
- **Beta 测试**：给外部真实用户试用，收集真实环境反馈。
- **UAT（用户验收测试）**：用户/客户确认是否满足业务需求与验收标准。
- UAT 重点是 **业务可接受**，不一定追求找出最多 bug。

---
